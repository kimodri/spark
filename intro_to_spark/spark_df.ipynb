{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d332985f-1c11-4421-8c52-6dacf88a0903",
   "metadata": {},
   "source": [
    "# Introduction to Spark\n",
    "Some things about spark\n",
    "\n",
    "## Creating a Session\n",
    "To create a session\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"BootcampDay1\").getOrCreate()\n",
    "\n",
    "print(spark.version)\n",
    "```\n",
    "## Reading a `CSV` & DataFrames\n",
    "Some of the most common methods you are going to use to a dataframe includes:\n",
    "- `.select()`\n",
    "- `.filter()`\n",
    "- `.groupBy()`\n",
    "- `.agg()`\n",
    "  \n",
    "To read a csv, you simply\n",
    "```python\n",
    "df = spark.read.csv('x.csv', header=True, inferSchema=True)\n",
    "df.show()\n",
    "```\n",
    "- To inspect the schema: `df.printSchema()`\n",
    "- To count the number of rows: `row_count = df.count()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceab59d-7140-4f8a-aefd-d88051dbcc56",
   "metadata": {},
   "source": [
    "### Aggregating Tables\n",
    "You can use `groupBy()` to aggregate the table:\n",
    "```python\n",
    "df.groupBy('gender').agg({'salary': 'avg'}).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc358534-558e-42a6-9ffc-c93139708422",
   "metadata": {},
   "source": [
    "### Filtering a Table\n",
    "```python\n",
    "filtered_df = df.filter(df['age'] > 50).select('age', 'occupation')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42c1126-682f-4c20-95be-1a4a482e9c42",
   "metadata": {},
   "source": [
    "## More DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3decf879-45d4-4e89-a38d-3e772f697045",
   "metadata": {},
   "source": [
    "To read csv: `spark.read.csv(csv)`\n",
    "\n",
    "To read json: `spark.read.json('json')`\n",
    "\n",
    "To read parquet: `spark.read.parquet('parquet')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eef23d-318d-468f-b418-1b8948c0b5b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d09be011-a7b0-4e47-ad15-39a7fa05791c",
   "metadata": {},
   "source": [
    "### Manually Configuring a Schema\n",
    "- We must specify the data type of the columns\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType\n",
    "\n",
    "# construct the schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"scores\", ArrayTyoe(IntegerType()), True),\n",
    "])\n",
    "\n",
    "# set the schema\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c5626-394b-4635-881a-9bb200a223b1",
   "metadata": {},
   "source": [
    "### Filtering and Selection\n",
    "- Use `.select()` to choose specific columns\n",
    "- Use `.filter()` or `.where()` to filter rows based on conditions\n",
    "- Use `.sort()` or orderBy() to order by a collection of columns\n",
    "\n",
    "```python\n",
    "df.select(\"name\", \"age\").show()\n",
    "\n",
    "df.filter(df['age'] > 30).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f80e4d-ec48-425f-a305-98b278965805",
   "metadata": {},
   "source": [
    "### Dropping Missing Values\n",
    "`df.na.drop().show()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0fb6e4-2a47-4f60-853d-1bb97e37e5cc",
   "metadata": {},
   "source": [
    "# Diving More to DataFrames & Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84632627-51b6-44f9-8222-d856b229d75b",
   "metadata": {},
   "source": [
    "### 1\\. Solving Null Values\n",
    "\n",
    "#### *Dropping Null Values*\n",
    "\n",
    "You can remove rows that contain null values using the `.drop()` method.\n",
    "\n",
    "| Method | Description | Example |\n",
    "| :--- | :--- | :--- |\n",
    "| **`.drop()`** | Drops rows where *any* column is null. | `df.na.drop()` |\n",
    "| **`.drop('any')`** | Alias for the default behavior. | `df.na.drop('any')` |\n",
    "| **`.drop('all')`** | Drops rows only if *all* columns are null. | `df.na.drop('all')` |\n",
    "| **`.drop(subset=['col1', 'col2'])`** | Drops rows where nulls appear in *specified* columns. | `df.na.drop(subset=['Age', 'Income'])` |\n",
    "\n",
    "-----\n",
    "\n",
    "#### *Filling Null Values*\n",
    "\n",
    "You can replace null values using the `.fill()` method. This is often preferred over dropping data, especially with numerical columns.\n",
    "\n",
    "| Method | Description | Example |\n",
    "| :--- | :--- | :--- |\n",
    "| **`.fill(value)`** | Fills nulls in all columns of the same type as `value`. | `df.na.fill(0)` |\n",
    "| **`.fill(value, subset=[...])`** | Fills nulls in specified columns with `value`. | `df.na.fill('Unknown', ['Name'])` |\n",
    "| **Advanced Filling** | Use `mean`, `median`, or `mode` to fill nulls, often calculated using `avg()` and then passed to `.fill()`. | `mean_age = df.agg({'Age': 'avg'}).collect()[0][0]` <br> `df.na.fill(mean_age, ['Age'])` |\n",
    "\n",
    "### 2\\. Column Operations: `withColumn`, `withColumnRenamed`, and `drop`\n",
    "\n",
    "These operations are used to transform, add, rename, and remove columns from a DataFrame.\n",
    "\n",
    "-----\n",
    "\n",
    "#### *Adding or Transforming Columns with `withColumn`*\n",
    "\n",
    "The `.withColumn()` method is used to add a new column or replace an existing one. It requires a column name and a column expression (often created using the **`F.col`** or **`F.lit`** functions from `pyspark.sql.functions`).\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Add a new column 'Salary_USD' by multiplying 'Salary' by a literal value\n",
    "df = df.withColumn('Salary_USD', F.col('Salary') * 0.75)\n",
    "\n",
    "# Create a new column 'Is_Senior' based on a condition\n",
    "df = df.withColumn('Is_Senior', F.when(F.col('Age') > 45, 'Yes').otherwise('No'))\n",
    "```\n",
    "\n",
    "#### *Renaming Columns with `withColumnRenamed`*\n",
    "\n",
    "The `.withColumnRenamed()` method is straightforward for changing a column's name.\n",
    "\n",
    "```python\n",
    "# Rename the column 'ID' to 'Employee_ID'\n",
    "df = df.withColumnRenamed('ID', 'Employee_ID')\n",
    "```\n",
    "\n",
    "#### *Removing Columns with `drop`*\n",
    "\n",
    "The `.drop()` method removes one or more specified columns.\n",
    "\n",
    "```python\n",
    "# Remove a single column\n",
    "df = df.drop('Temp_Column')\n",
    "\n",
    "# Remove multiple columns\n",
    "df = df.drop('col1', 'col2', 'col3')\n",
    "```\n",
    "\n",
    "### 3\\. Row Operations: `filter` and `groupBy`\n",
    "\n",
    "These operations are crucial for selecting subsets of data and summarizing information.\n",
    "\n",
    "-----\n",
    "\n",
    "#### *Selecting Rows with `filter` (or `where`)*\n",
    "\n",
    "The `.filter()` method (or its alias `.where()`) selects rows based on a specified condition. The condition is typically written using column expressions.\n",
    "\n",
    "```python\n",
    "# Filter rows where 'Age' is greater than 30\n",
    "df_adults = df.filter(F.col('Age') > 30)\n",
    "\n",
    "# Filter rows where 'Department' is 'Sales' AND 'Salary' is greater than 50000\n",
    "df_sales_high = df.filter((F.col('Department') == 'Sales') & (F.col('Salary') > 50000))\n",
    "```\n",
    "\n",
    "**Note:** Use the `&` (AND) and `|` (OR) operators for combining multiple conditions, and ensure conditions are enclosed in parentheses.\n",
    "\n",
    "#### *Grouping and Aggregating with `groupBy`*\n",
    "\n",
    "The `.groupBy()` method groups the DataFrame by one or more columns, typically followed by an aggregation function to calculate summary statistics for each group.\n",
    "\n",
    "```python\n",
    "# Calculate the average salary for each department\n",
    "df_summary = df.groupBy('Department').agg(\n",
    "    F.count('*').alias('Employee_Count'), # Counts the number of rows in each group\n",
    "    F.avg('Salary').alias('Average_Salary'), # Calculates the mean salary\n",
    "    F.max('Age').alias('Max_Age')\n",
    ").orderBy('Average_Salary', ascending=False)\n",
    "```\n",
    "\n",
    "The aggregation function is applied using the `.agg()` method, which can handle multiple aggregations at once. Common aggregation functions include `F.count`, `F.sum`, `F.avg`, `F.min`, and `F.max`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f25319-4ca7-4ce8-9e1b-dbd3f0fda4f6",
   "metadata": {},
   "source": [
    "## Advance DataFrame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db91b4e7-eed6-4956-acbc-b307790d1998",
   "metadata": {},
   "source": [
    "### 1\\. Joining DataFrames\n",
    "\n",
    "Joining DataFrames combines them based on common key columns. PySpark supports joins using column equality expressions.\n",
    "\n",
    "-----\n",
    "\n",
    "#### Joining with the `on` Parameter (Recommended)\n",
    "\n",
    "The `on` parameter is the standard and often clearer way to specify the join condition. It accepts either a **string** or a **list of strings** containing the names of the common column(s) in both DataFrames.\n",
    "\n",
    "  * **Behavior:** When using `on`, PySpark automatically handles columns with the same name, resulting in only **one copy** of the join key column in the output.\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "# Assuming both df_A and df_B have a column named 'ID'\n",
    "df_joined = df_A.join(df_B, on='ID', how='inner')\n",
    "\n",
    "# Joining on multiple columns: 'ID' and 'Dept'\n",
    "df_joined_multi = df_A.join(df_B, on=['ID', 'Dept'], how='left')\n",
    "```\n",
    "\n",
    "#### Joining with an Equality Condition (`==`)\n",
    "\n",
    "You can define a join condition using a standard **Boolean expression** formed by comparing columns from the two DataFrames. This is necessary when the join key columns have **different names**.\n",
    "\n",
    "  * **Behavior:** When using an explicit condition, if the joining columns have the same name, they will appear **twice** in the output (e.g., `df_A.ID` and `df_B.ID`). If they have different names, they will both be present.\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Joining df_A using 'A_Key' and df_B using 'B_Key'\n",
    "df_joined_expr = df_A.join(\n",
    "    df_B,\n",
    "    df_A['A_Key'] == df_B['B_Key'],\n",
    "    how='inner'\n",
    ")\n",
    "```\n",
    "\n",
    "### 2\\. Union (Combining DataFrames)\n",
    "\n",
    "The `.union()` operation combines the rows of two or more DataFrames. For the union to succeed, the DataFrames must have the **same schema** (the same number of columns, and corresponding columns must have compatible data types).\n",
    "\n",
    "-----\n",
    "\n",
    "#### Simple Union with Identical Schema\n",
    "\n",
    "If two DataFrames, `df1` and `df2`, have an identical column structure, you can simply use `.union()`:\n",
    "\n",
    "```python\n",
    "# df1 and df2 must have the exact same column names and types in the same order\n",
    "df_combined = df1.union(df2)\n",
    "```\n",
    "\n",
    "#### Union by Name (`unionByName`)\n",
    "\n",
    "If the DataFrames have the same columns but they are in a **different order**, use `.unionByName()`:\n",
    "\n",
    "```python\n",
    "# df1 has ['ColA', 'ColB'], df2 has ['ColB', 'ColA']\n",
    "df_combined_by_name = df1.unionByName(df2)\n",
    "\n",
    "# To handle cases where one DataFrame might have extra columns\n",
    "# use allowMissingColumns=True (but be cautious, as missing columns will be filled with nulls)\n",
    "df_combined_strict = df1.unionByName(df2, allowMissingColumns=False)\n",
    "```\n",
    "\n",
    "### 3\\. Working with Complex Types: Arrays and Maps\n",
    "\n",
    "PySpark supports complex nested data types like Arrays (lists of elements) and Maps (key-value pairs) which are useful for semi-structured data.\n",
    "\n",
    "-----\n",
    "\n",
    "#### Creating and Working with Arrays\n",
    "\n",
    "An Array column holds a sequence of values of the same type.\n",
    "\n",
    "**Example Setup (Using sample from prompt):**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "# Create an array column named 'scores'\n",
    "df = df.withColumn(\"scores\", F.array(F.lit(85), F.lit(90), F.lit(78)))\n",
    "```\n",
    "\n",
    "| Operation | Function | Description | Example |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Explode** | `F.explode()` | Converts each element of an array into a separate row. | `df.withColumn('score', F.explode('scores'))` |\n",
    "| **Size** | `F.size()` | Returns the number of elements in the array. | `df.withColumn('count', F.size('scores'))` |\n",
    "| **Element Access** | `F.col()[index]` | Accesses an element by its zero-based index. | `df.withColumn('first', F.col('scores')[0])` |\n",
    "\n",
    "#### Creating and Working with Maps\n",
    "\n",
    "A Map column holds key-value pairs, similar to a Python dictionary.\n",
    "\n",
    "**Example Setup (Schema definition from prompt):**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import MapType, StringType, StructField\n",
    "# Define a schema with a MapType column\n",
    "map_schema = StructField('properties', MapType(StringType(), StringType()), True)\n",
    "# Create a map column (requires creating a map from key-value pairs)\n",
    "df = df.withColumn(\"attributes\", F.create_map(F.lit('color'), F.lit('red'), F.lit('size'), F.lit('large')))\n",
    "```\n",
    "\n",
    "| Operation | Function | Description | Example |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Key Access** | `F.col()['key']` | Accesses the value associated with a specific key. | `df.withColumn('color', F.col('attributes')['color'])` |\n",
    "| **Keys/Values** | `F.map_keys()`, `F.map_values()` | Returns an array containing all keys or all values in the map. | `df.withColumn('keys', F.map_keys('attributes'))` |\n",
    "\n",
    "### 4\\. Working with Structs\n",
    "\n",
    "A **StructType** (or **Struct**) is a way to group related columns into a single nested column, similar to a record or an object in other languages.\n",
    "\n",
    "-----\n",
    "\n",
    "#### Creating a Struct\n",
    "\n",
    "You can create a Struct using the `F.struct()` function, which combines several columns into one.\n",
    "\n",
    "```python\n",
    "# Group 'firstName' and 'lastName' into a single 'Name' struct column\n",
    "df_struct = df.withColumn('Name', F.struct('firstName', 'lastName'))\n",
    "\n",
    "# The resulting schema: Name.firstName and Name.lastName\n",
    "```\n",
    "\n",
    "#### Accessing Elements in a Struct\n",
    "\n",
    "Accessing data inside a Struct is done using **dot notation**.\n",
    "\n",
    "```python\n",
    "# Access the 'lastName' field inside the 'Name' struct\n",
    "df_lastName = df_struct.withColumn('Last', F.col('Name.lastName'))\n",
    "\n",
    "# You can also use select\n",
    "df_select = df_struct.select(F.col('Name.*')) # Expands all fields in the struct\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767eade1-a593-487e-b91d-99b572485f52",
   "metadata": {},
   "source": [
    "## UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c6964d-d58b-4f9a-a683-0bf8d88ca728",
   "metadata": {},
   "source": [
    "### 1\\. Standard PySpark UDF (User-Defined Function)\n",
    "\n",
    "A **standard UDF** applies a Python function **row-by-row** to a PySpark DataFrame.\n",
    "\n",
    "| Feature | Description |\n",
    "| :--- | :--- |\n",
    "| **Execution** | **Row-by-Row.** The function is called for every single row in the DataFrame. |\n",
    "| **Data Transfer** | **High Overhead.** Data is individually **serialized** (converted from Spark's internal format to Python's) and **deserialized** (converted back) for every single row, causing significant communication overhead. |\n",
    "| **Performance** | **Slow.** The serialization cost and single-row execution limit performance. It also hinders Spark's Catalyst Optimizer. |\n",
    "| **Syntax** | Defined using `from pyspark.sql.functions import udf`. You **must** explicitly specify the **return type**. |\n",
    "| **Best Used For** | Logic that is too complex or impossible to express using built-in Spark functions, especially on **small-to-medium datasets** where the overhead is negligible. |\n",
    "\n",
    "#### Example\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# 1. Define the Python function\n",
    "def custom_greet(name):\n",
    "    if name is None:\n",
    "        return None\n",
    "    return \"Hello, \" + name.upper()\n",
    "\n",
    "# 2. Convert to a PySpark UDF\n",
    "greet_udf = udf(custom_greet, StringType())\n",
    "\n",
    "# 3. Apply the UDF\n",
    "df.withColumn(\"Greeting\", greet_udf(df['Name'])).show()\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "### 2\\. Pandas UDF (Vectorized UDF)\n",
    "\n",
    "A **Pandas UDF** applies a Python function **batch-by-batch** using Pandas Series or DataFrames. It is often referred to as a **Vectorized UDF**.\n",
    "\n",
    "| Feature | Description |\n",
    "| :--- | :--- |\n",
    "| **Execution** | **Batch/Vectorized.** Spark splits the data into batches, and the function operates on the entire batch at once as a Pandas Series (column) or DataFrame. |\n",
    "| **Data Transfer** | **Low Overhead.** Uses **Apache Arrow** for efficient, columnar data transfer between the Spark (JVM) and Python execution environments, significantly reducing serialization cost. |\n",
    "| **Performance** | **Much Faster** than standard UDFs (potentially up to 100x). It leverages the **vectorized operations** and optimized C/C++ backend of Pandas and NumPy. |\n",
    "| **Syntax** | Defined using the `@pandas_udf` decorator. Requires specifying the **input and output types** using Python type hints (e.g., `pd.Series`) and the return type in the decorator. |\n",
    "| **Best Used For** | Complex logic that benefits from **Pandas/NumPy vectorized operations** and performing group-wise transformations on **large datasets**. |\n",
    "\n",
    "#### Example (Series to Series - Scalar UDF)\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# 1. Define the Python function using type hints\n",
    "@pandas_udf(DoubleType())\n",
    "def standardize_score(scores: pd.Series) -> pd.Series:\n",
    "    # Operations are applied to the entire Pandas Series at once (vectorized)\n",
    "    return (scores - scores.mean()) / scores.std()\n",
    "\n",
    "# 2. Apply the UDF\n",
    "df.withColumn(\"Standardized_Score\", standardize_score(df['Score'])).show()\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "### Performance Hierarchy\n",
    "\n",
    "For optimal performance in PySpark, you should prioritize your custom logic implementation in this order:\n",
    "\n",
    "1.  **Spark Built-in Functions** (`pyspark.sql.functions`): **Fastest.** Use these first, as they are natively compiled and fully optimized by Spark.\n",
    "2.  **Pandas UDF (Vectorized)**: **Much Faster** than standard UDFs. Use for complex logic that benefits from Pandas/NumPy.\n",
    "3.  **Standard PySpark UDF**: **Slowest.** Use only as a last resort when the logic cannot be expressed using the other two methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2c943-13bb-4134-8881-6e6f60c14c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
